{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52886b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "875c0dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Using device: cuda ---\n",
      "Data transforms loaded for vit_base_patch16_224.augreg_in21k.\n",
      "Data loaded: 22788 training samples, 5696 validation samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\macro-estimator\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC\\.cache\\huggingface\\hub\\models--timm--vit_base_patch16_224.augreg_in21k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, Loss function, and Optimizer are ready.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script fine-tunes a Vision Transformer (ViT-B/16) pre-trained on ImageNet-21k\n",
    "for a regression task: predicting nutritional values from an image of a dish.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Any\n",
    "import timm\n",
    "\n",
    "from src.macro_estimator.models.vit_regressor import ViTRegressor\n",
    "from src.macro_estimator.datasets import Nutrition5kDataset\n",
    "\n",
    "# --- 1. Configuration and Constants ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Data Paths ---\n",
    "IMAGES_CSV_PATH = Path(\"data/csv_files/images.csv\")\n",
    "LABELS_CSV_PATH = Path(\"data/csv_files/labels.csv\")\n",
    "MODEL_SAVE_PATH = Path(\"artifacts/models/vit_nutrition_regressor.pth\")\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 16  # Adjust based on your GPU memory\n",
    "EPOCHS = 20      # Fine-tuning might require more epochs\n",
    "WEIGHT_DECAY = 1e-4\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "def main():\n",
    "    print(f\"--- Using device: {DEVICE} ---\")\n",
    "\n",
    "    # --- CORRECCIÓN: Usar el nombre completo del modelo ---\n",
    "    MODEL_NAME = 'vit_base_patch16_224.augreg_in21k'\n",
    "\n",
    "    # --- Data Loading and Transformations ---\n",
    "    # timm es inteligente. Si el nombre del modelo ya especifica los pesos,\n",
    "    # no necesita 'pretrained=True' aquí.\n",
    "    temp_model = timm.create_model(MODEL_NAME)\n",
    "    data_config = timm.data.resolve_data_config(model=temp_model)\n",
    "    transforms = timm.data.create_transform(**data_config)\n",
    "    print(f\"Data transforms loaded for {MODEL_NAME}.\")\n",
    "\n",
    "    # ... (El código de Dataset y DataLoader no cambia) ...\n",
    "    full_dataset = Nutrition5kDataset(\n",
    "        images_csv_path=IMAGES_CSV_PATH,\n",
    "        labels_csv_path=LABELS_CSV_PATH,\n",
    "        transform=transforms\n",
    "    )\n",
    "    # ... (split y DataLoaders) ...\n",
    "    val_size = int(len(full_dataset) * VAL_SPLIT)\n",
    "    train_size = len(full_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    print(f\"Data loaded: {train_size} training samples, {val_size} validation samples.\")\n",
    "\n",
    "    # --- Model, Loss, and Optimizer ---\n",
    "    # --- CORRECCIÓN: Pasar el nombre correcto del modelo ---\n",
    "    # La clase ViTRegressor no necesita cambios, solo le pasamos el nombre correcto.\n",
    "    model = ViTRegressor(model_name=MODEL_NAME, n_outputs=4).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    print(\"Model, Loss function, and Optimizer are ready.\")\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macro-estimator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
